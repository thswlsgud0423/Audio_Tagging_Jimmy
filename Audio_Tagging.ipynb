{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# file_path\n",
    "audio_path = Path(\"dataset\") / \"audio\"\n",
    "csv_path = Path(\"dataset\") / \"esc50.csv\"\n",
    "\n",
    "metadata = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "**Sample rate** - 22050 Hz\n",
    "\n",
    "**Duration** - 5 sec\n",
    "\n",
    "**MFCC** - 13 coefficients are extracted per frame (But I am not sure if i will use this)\n",
    "\n",
    "**Mel Bands** - 128 mel bands are used to compute the Mel spectrogram.\n",
    "\n",
    "    Mel bands are frequency bands derived from the Mel scale, which mimics human auditory perception.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "I simply used the libraries to extract mfcc and mel_spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsr = 22050\\nduration = 5 # ESC50 has 5 sec duration\\nn_mfcc = 13\\nn_mels = 128\\n\\n# Output\\nmfcc_dir = Path(\"processed_data\") / \"mfcc\"\\nmel_dir = Path(\"processed_data\") / \"mel_spectrogram\"\\nmfcc_dir.mkdir(parents=True, exist_ok=True)\\nmel_dir.mkdir(parents=True, exist_ok=True)\\n\\n# Function to save MFCC and Mel Spectrogram\\ndef save_features(audio_path, filename, sr=22050):\\n    # Load audio\\n    y, _ = librosa.load(audio_path, sr=sr, duration=duration)\\n\\n    # Compute MFCC and Mel_spectrogram\\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\\n    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\\n\\n    # Save as .npy\\n    np.save(f\"processed_data/mfcc/{filename}_mfcc.npy\", mfcc)\\n    np.save(f\"processed_data/mel_spectrogram/{filename}_mel_spectrogram.npy\", mel_spectrogram)\\n\\n# Save features\\nfor _, row in metadata.iterrows():\\n    audio_file = f\"dataset/audio/{row[\\'filename\\']}\"\\n    filename = row[\"filename\"].split(\".\")[0]\\n    save_features(audio_file, filename) '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sr = 22050\n",
    "duration = 5 # ESC50 has 5 sec duration\n",
    "n_mfcc = 13\n",
    "n_mels = 128\n",
    "\n",
    "# Output\n",
    "mfcc_dir = Path(\"processed_data\") / \"mfcc\"\n",
    "mel_dir = Path(\"processed_data\") / \"mel_spectrogram\"\n",
    "mfcc_dir.mkdir(parents=True, exist_ok=True)\n",
    "mel_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Function to save MFCC and Mel Spectrogram\n",
    "def save_features(audio_path, filename, sr=22050):\n",
    "    # Load audio\n",
    "    y, _ = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "\n",
    "    # Compute MFCC and Mel_spectrogram\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "\n",
    "    # Save as .npy\n",
    "    np.save(f\"processed_data/mfcc/{filename}_mfcc.npy\", mfcc)\n",
    "    np.save(f\"processed_data/mel_spectrogram/{filename}_mel_spectrogram.npy\", mel_spectrogram)\n",
    "\n",
    "# Save features\n",
    "for _, row in metadata.iterrows():\n",
    "    audio_file = f\"dataset/audio/{row['filename']}\"\n",
    "    filename = row[\"filename\"].split(\".\")[0]\n",
    "    save_features(audio_file, filename) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata = pd.read_csv(\"dataset/esc50.csv\")\n",
    "\n",
    "# Function to load features and flatten them\n",
    "def load_features(feature_type=\"mfcc\"):\n",
    "    feature_dir = Path(f\"processed_data/{feature_type}\")\n",
    "    X, y = [], []\n",
    "    for _, row in metadata.iterrows():\n",
    "        class_label = row[\"category\"]\n",
    "        file_name = row[\"filename\"].split(\".\")[0]\n",
    "        \n",
    "        # Load the file\n",
    "        feature_path = feature_dir / f\"{file_name}_{feature_type}.npy\"\n",
    "        features = np.load(feature_path)\n",
    "        \n",
    "        # Flatten the features to 1D\n",
    "        X.append(features)\n",
    "        y.append(class_label)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load and split data for MFCC\n",
    "X_mfcc, y_mfcc = load_features(\"mfcc\")\n",
    "X_train_mfcc, X_test_mfcc, y_train_mfcc, y_test_mfcc = train_test_split(X_mfcc, y_mfcc, test_size=0.2, random_state=12)\n",
    "\n",
    "# for Mel Spectrogram\n",
    "X_mel, y_mel = load_features(\"mel_spectrogram\")\n",
    "X_train_mel, X_test_mel, y_train_mel, y_test_mel = train_test_split(X_mel, y_mel, test_size=0.2, random_state=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Mel spectrogram: (128, 216)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of Mel spectrogram: {X_mel[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features_ml(feature_type=\"mfcc\"):\n",
    "    feature_dir = Path(f\"processed_data/{feature_type}\")\n",
    "    X, y = [], []\n",
    "    for _, row in metadata.iterrows():\n",
    "        class_label = row[\"category\"]\n",
    "        file_name = row[\"filename\"].split(\".\")[0]\n",
    "        \n",
    "        # Load the feature file\n",
    "        feature_path = feature_dir / f\"{file_name}_{feature_type}.npy\"\n",
    "        features = np.load(feature_path)\n",
    "        \n",
    "        # Flatten the features (e.g., 2D to 1D)\n",
    "        X.append(features.flatten())  # Flatten here\n",
    "        y.append(class_label)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# for Mel Spectrogram\n",
    "X_mel_ml, y_mel_ml = load_features_ml(\"mel_spectrogram\")\n",
    "X_train_mel_ml, X_test_mel_ml, y_train_mel_ml, y_test_mel_ml = train_test_split(X_mel_ml, y_mel_ml, test_size=0.2, random_state=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating models for Mel Spectrogram features:\n",
      "\n",
      "KNN Model Accuracy: 0.09\n",
      "Random Forest Model Accuracy: 0.34\n",
      "SVM Model Accuracy: 0.14\n"
     ]
    }
   ],
   "source": [
    "# ML Models \n",
    "\n",
    "# Initialize models\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=12)\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Model List\n",
    "models = {\"KNN\": knn, \"Random Forest\": rf, \"SVM\": svm}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, feature_type):\n",
    "    print(f\"\\nEvaluating models for {feature_type} features:\\n\")\n",
    "    for name, model in models.items():\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluation\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"{name} Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\"\"\"\n",
    "# MFCC\n",
    "train_and_evaluate(X_train_mfcc, X_test_mfcc, y_train_mfcc, y_test_mfcc, \"MFCC\")\n",
    "\"\"\"\n",
    "\n",
    "# Mel Spectrogram\n",
    "train_and_evaluate(X_train_mel_ml, X_test_mel_ml, y_train_mel_ml, y_test_mel_ml, \"Mel Spectrogram\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        dummy_input = torch.zeros(1, 1, 128, 216)  # (batch_size, channels, height, width)\n",
    "        out = self.pool(self.relu(self.conv1(dummy_input)))\n",
    "        out = self.pool(self.relu(self.conv2(out)))\n",
    "        self.flattened_size = out.numel()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Dataset\n",
    "\n",
    "This class handles audio feature datasets: load, normalize, and return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, metadata, feature_type, feature_dir, num_classes):\n",
    "        self.metadata = metadata\n",
    "        self.feature_type = feature_type\n",
    "        self.feature_dir = feature_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.label_map = {label: idx for idx, label in enumerate(metadata['category'].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        file_name = row[\"filename\"].split(\".\")[0]\n",
    "        feature_path = f\"{self.feature_dir}/{file_name}_{self.feature_type}.npy\"\n",
    "        features = np.load(feature_path)\n",
    "\n",
    "        # Normalize features and add channel dimension\n",
    "        features = (features - np.mean(features)) / np.std(features)\n",
    "        features = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        label = self.label_map[row[\"category\"]]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return features, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Simple model that computes loss, clears gradients from the previous step, computes gradients wrt the loss, and updates the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_loader, test_loader, device, num_epochs=25):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "        epoch_accuracy = (correct_predictions / total_predictions) * 100  # in percentage\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRNN Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try RNN and CNN next time\n",
    "\n",
    "class ImprovedCRNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, img_height=128, img_width=216, num_classes=50,\n",
    "                 map_to_seq_hidden=128, rnn_hidden_size=216, num_rnn_layers=3, dropout=0.3):\n",
    "        super(ImprovedCRNN, self).__init__()\n",
    "        \n",
    "        # CNN backbone\n",
    "        self.cnn, (output_channels, output_height, output_width) = self._cnn_backbone(\n",
    "            input_channels, img_height, img_width\n",
    "        )\n",
    "        \n",
    "        # Map CNN output to sequence\n",
    "        self.map_to_seq = nn.Linear(output_channels * output_height, map_to_seq_hidden)\n",
    "        \n",
    "        # Recurrent layers\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            map_to_seq_hidden,\n",
    "            rnn_hidden_size,\n",
    "            num_layers=num_rnn_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(rnn_hidden_size * 2, num_classes)\n",
    "\n",
    "    def _cnn_backbone(self, input_channels, img_height, img_width):\n",
    "        \n",
    "        cnn = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # Halves height and width\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # Halves height and width\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1)),  # Halves height, keeps width\n",
    "            nn.Dropout(0.6),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1)),  # Halves height, keeps width\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # Calculate output shape\n",
    "        final_height = img_height // (2 * 2 * 2 * 2)  # Height halved 4 times\n",
    "        final_width = img_width // (2 * 2)  # Width halved 2 times\n",
    "        output_shape = (512, final_height, final_width)\n",
    "\n",
    "        return cnn, output_shape\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.cnn(x)\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        x = x.view(batch_size, channels * height, width).permute(2, 0, 1)\n",
    "        x = self.map_to_seq(x)\n",
    "        x, _ = self.rnn1(x)\n",
    "        x = self.fc(x[-1])\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CRNN...\n",
      "Epoch [1/100], Loss: 3.7136, Accuracy: 4.88%\n",
      "Epoch [2/100], Loss: 3.2738, Accuracy: 10.69%\n",
      "Epoch [3/100], Loss: 3.0864, Accuracy: 15.00%\n",
      "Epoch [4/100], Loss: 2.9489, Accuracy: 17.06%\n",
      "Epoch [5/100], Loss: 2.8244, Accuracy: 18.06%\n",
      "Epoch [6/100], Loss: 2.7409, Accuracy: 19.81%\n",
      "Epoch [7/100], Loss: 2.6763, Accuracy: 21.12%\n",
      "Epoch [8/100], Loss: 2.5533, Accuracy: 23.88%\n",
      "Epoch [9/100], Loss: 2.4461, Accuracy: 26.31%\n",
      "Epoch [10/100], Loss: 2.3366, Accuracy: 28.75%\n",
      "Epoch [11/100], Loss: 2.3105, Accuracy: 31.00%\n",
      "Epoch [12/100], Loss: 2.2112, Accuracy: 31.06%\n",
      "Epoch [13/100], Loss: 2.1023, Accuracy: 35.75%\n",
      "Epoch [14/100], Loss: 2.0466, Accuracy: 37.25%\n",
      "Epoch [15/100], Loss: 2.0033, Accuracy: 38.69%\n",
      "Epoch [16/100], Loss: 1.9896, Accuracy: 37.69%\n",
      "Epoch [17/100], Loss: 1.9111, Accuracy: 41.25%\n",
      "Epoch [18/100], Loss: 1.7948, Accuracy: 44.88%\n",
      "Epoch [19/100], Loss: 1.6896, Accuracy: 46.44%\n",
      "Epoch [20/100], Loss: 1.7003, Accuracy: 45.44%\n",
      "Epoch [21/100], Loss: 1.5822, Accuracy: 50.56%\n",
      "Epoch [22/100], Loss: 1.6287, Accuracy: 49.12%\n",
      "Epoch [23/100], Loss: 1.5937, Accuracy: 51.00%\n",
      "Epoch [24/100], Loss: 1.4328, Accuracy: 55.25%\n",
      "Epoch [25/100], Loss: 1.3882, Accuracy: 54.87%\n",
      "Epoch [26/100], Loss: 1.3719, Accuracy: 56.25%\n",
      "Epoch [27/100], Loss: 1.3207, Accuracy: 57.69%\n",
      "Epoch [28/100], Loss: 1.1929, Accuracy: 62.12%\n",
      "Epoch [29/100], Loss: 1.1908, Accuracy: 62.94%\n",
      "Epoch [30/100], Loss: 1.1555, Accuracy: 63.44%\n",
      "Epoch [31/100], Loss: 1.1434, Accuracy: 63.56%\n",
      "Epoch [32/100], Loss: 1.0807, Accuracy: 64.50%\n",
      "Epoch [33/100], Loss: 0.9769, Accuracy: 69.06%\n",
      "Epoch [34/100], Loss: 0.9115, Accuracy: 70.62%\n",
      "Epoch [35/100], Loss: 0.8686, Accuracy: 72.19%\n",
      "Epoch [36/100], Loss: 0.9309, Accuracy: 68.38%\n",
      "Epoch [37/100], Loss: 0.7866, Accuracy: 75.62%\n",
      "Epoch [38/100], Loss: 0.7845, Accuracy: 74.62%\n",
      "Epoch [39/100], Loss: 0.7312, Accuracy: 76.81%\n",
      "Epoch [40/100], Loss: 0.7580, Accuracy: 76.12%\n",
      "Epoch [41/100], Loss: 0.7530, Accuracy: 75.69%\n",
      "Epoch [42/100], Loss: 0.7767, Accuracy: 75.88%\n",
      "Epoch [43/100], Loss: 0.7537, Accuracy: 74.88%\n",
      "Epoch [44/100], Loss: 0.6299, Accuracy: 79.44%\n",
      "Epoch [45/100], Loss: 0.6061, Accuracy: 79.94%\n",
      "Epoch [46/100], Loss: 0.5235, Accuracy: 84.62%\n",
      "Epoch [47/100], Loss: 0.5284, Accuracy: 83.44%\n",
      "Epoch [48/100], Loss: 0.4973, Accuracy: 84.88%\n",
      "Epoch [49/100], Loss: 0.4280, Accuracy: 86.81%\n",
      "Epoch [50/100], Loss: 0.3890, Accuracy: 88.25%\n",
      "Epoch [51/100], Loss: 0.4138, Accuracy: 87.62%\n",
      "Epoch [52/100], Loss: 0.3984, Accuracy: 87.62%\n",
      "Epoch [53/100], Loss: 0.4035, Accuracy: 87.31%\n",
      "Epoch [54/100], Loss: 0.4114, Accuracy: 86.81%\n",
      "Epoch [55/100], Loss: 0.4350, Accuracy: 86.38%\n",
      "Epoch [56/100], Loss: 0.4319, Accuracy: 85.44%\n",
      "Epoch [57/100], Loss: 0.4174, Accuracy: 85.88%\n",
      "Epoch [58/100], Loss: 0.3812, Accuracy: 87.75%\n",
      "Epoch [59/100], Loss: 0.3488, Accuracy: 88.94%\n",
      "Epoch [60/100], Loss: 0.3031, Accuracy: 91.50%\n",
      "Epoch [61/100], Loss: 0.2737, Accuracy: 92.12%\n",
      "Epoch [62/100], Loss: 0.2831, Accuracy: 90.69%\n",
      "Epoch [63/100], Loss: 0.3082, Accuracy: 90.44%\n",
      "Epoch [64/100], Loss: 0.3626, Accuracy: 88.81%\n",
      "Epoch [65/100], Loss: 0.3124, Accuracy: 89.94%\n",
      "Epoch [66/100], Loss: 0.3480, Accuracy: 89.81%\n",
      "Epoch [67/100], Loss: 0.2875, Accuracy: 91.06%\n",
      "Epoch [68/100], Loss: 0.3701, Accuracy: 88.06%\n",
      "Epoch [69/100], Loss: 0.3148, Accuracy: 90.38%\n",
      "Epoch [70/100], Loss: 0.3096, Accuracy: 90.94%\n",
      "Epoch [71/100], Loss: 0.3108, Accuracy: 89.50%\n",
      "Epoch [72/100], Loss: 0.2570, Accuracy: 91.75%\n",
      "Epoch [73/100], Loss: 0.2192, Accuracy: 93.25%\n",
      "Epoch [74/100], Loss: 0.1873, Accuracy: 94.69%\n",
      "Epoch [75/100], Loss: 0.1769, Accuracy: 94.50%\n",
      "Epoch [76/100], Loss: 0.1908, Accuracy: 93.81%\n",
      "Epoch [77/100], Loss: 0.1990, Accuracy: 94.19%\n",
      "Epoch [78/100], Loss: 0.2099, Accuracy: 93.25%\n",
      "Epoch [79/100], Loss: 0.2964, Accuracy: 90.81%\n",
      "Epoch [80/100], Loss: 0.2583, Accuracy: 92.00%\n",
      "Epoch [81/100], Loss: 0.2005, Accuracy: 93.94%\n",
      "Epoch [82/100], Loss: 0.1894, Accuracy: 94.38%\n",
      "Epoch [83/100], Loss: 0.1379, Accuracy: 96.38%\n",
      "Epoch [84/100], Loss: 0.1708, Accuracy: 95.38%\n",
      "Epoch [85/100], Loss: 0.2095, Accuracy: 93.88%\n",
      "Epoch [86/100], Loss: 0.2239, Accuracy: 93.12%\n",
      "Epoch [87/100], Loss: 0.1985, Accuracy: 94.06%\n",
      "Epoch [88/100], Loss: 0.1836, Accuracy: 94.19%\n",
      "Epoch [89/100], Loss: 0.3372, Accuracy: 89.31%\n",
      "Epoch [90/100], Loss: 0.3983, Accuracy: 87.25%\n",
      "Epoch [91/100], Loss: 0.2946, Accuracy: 90.12%\n",
      "Epoch [92/100], Loss: 0.2095, Accuracy: 93.62%\n",
      "Epoch [93/100], Loss: 0.1983, Accuracy: 93.75%\n",
      "Epoch [94/100], Loss: 0.1799, Accuracy: 94.62%\n",
      "Epoch [95/100], Loss: 0.2000, Accuracy: 94.06%\n",
      "Epoch [96/100], Loss: 0.2104, Accuracy: 94.19%\n",
      "Epoch [97/100], Loss: 0.2431, Accuracy: 92.31%\n",
      "Epoch [98/100], Loss: 0.1703, Accuracy: 94.38%\n",
      "Epoch [99/100], Loss: 0.1342, Accuracy: 95.94%\n",
      "Epoch [100/100], Loss: 0.1134, Accuracy: 96.81%\n",
      "Training complete!\n",
      "CRNN Test Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    feature_type = \"mel_spectrogram\"\n",
    "    feature_dir = f\"processed_data/{feature_type}\"\n",
    "    metadata = pd.read_csv(\"dataset/esc50.csv\")\n",
    "    num_classes = len(metadata[\"category\"].unique())\n",
    "    num_epochs = 100\n",
    "\n",
    "    # Create dataset and dataloaders\n",
    "    dataset = AudioDataset(metadata, feature_type, feature_dir, num_classes)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ImprovedCRNN(input_channels=1, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    # Train SimpleCNN\n",
    "    print(\"Training SimpleCNN...\")\n",
    "    cnn_model = SimpleCNN(num_classes=num_classes).to(device)\n",
    "    train_model(cnn_model, train_loader, test_loader, device, num_epochs)\n",
    "\n",
    "    # Evaluate SimpleCNN\n",
    "    cnn_accuracy = evaluate_model(cnn_model, test_loader, device)\n",
    "    print(f\"SimpleCNN Test Accuracy: {cnn_accuracy:.2f}%\")\n",
    "    \"\"\"\n",
    "    # Train CRNN\n",
    "    print(\"\\nTraining CRNN...\")\n",
    "    crnn_model = ImprovedCRNN(input_channels=1, num_classes=num_classes).to(device)\n",
    "    train_model(crnn_model, train_loader, test_loader, device, num_epochs)\n",
    "\n",
    "    # Evaluate CRNN\n",
    "    crnn_accuracy = evaluate_model(crnn_model, test_loader, device)\n",
    "    print(f\"CRNN Test Accuracy: {crnn_accuracy:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jimmy_coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
