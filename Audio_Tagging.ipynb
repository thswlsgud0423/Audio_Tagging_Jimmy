{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# file_path\n",
    "audio_path = Path(\"dataset\") / \"audio\"\n",
    "csv_path = Path(\"dataset\") / \"esc50.csv\"\n",
    "\n",
    "metadata = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sr = 22050\n",
    "duration = 5 # ESC50 has 5 sec duration\n",
    "n_mfcc = 13\n",
    "n_mels = 128\n",
    "\n",
    "# Output\n",
    "mfcc_dir = Path(\"processed_data\") / \"mfcc\"\n",
    "mel_dir = Path(\"processed_data\") / \"mel_spectrogram\"\n",
    "mfcc_dir.mkdir(parents=True, exist_ok=True)\n",
    "mel_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Function to save MFCC and Mel Spectrogram\n",
    "def save_features(audio_path, filename, sr=22050):\n",
    "    # Load audio\n",
    "    y, _ = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "\n",
    "    # Compute MFCC and Mel_spectrogram\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "\n",
    "    # Save as .npy\n",
    "    np.save(f\"processed_data/mfcc/{filename}_mfcc.npy\", mfcc)\n",
    "    np.save(f\"processed_data/mel_spectrogram/{filename}_mel_spectrogram.npy\", mel_spectrogram)\n",
    "\n",
    "# Save features\n",
    "for _, row in metadata.iterrows():\n",
    "    audio_file = f\"dataset/audio/{row['filename']}\"\n",
    "    filename = row[\"filename\"].split(\".\")[0]\n",
    "    save_features(audio_file, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata = pd.read_csv(\"dataset/esc50.csv\")\n",
    "\n",
    "# Function to load features and flatten them\n",
    "def load_features(feature_type=\"mfcc\"):\n",
    "    feature_dir = Path(f\"processed_data/{feature_type}\")\n",
    "    X, y = [], []\n",
    "    for _, row in metadata.iterrows():\n",
    "        class_label = row[\"category\"]\n",
    "        file_name = row[\"filename\"].split(\".\")[0]\n",
    "        \n",
    "        # Load .npy file\n",
    "        feature_path = feature_dir / f\"{file_name}_{feature_type}.npy\"\n",
    "        features = np.load(feature_path)\n",
    "        \n",
    "        # Flatten the features to 1D for simple models\n",
    "        X.append(features.flatten())\n",
    "        y.append(class_label)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load and split data for MFCC\n",
    "X_mfcc, y_mfcc = load_features(\"mfcc\")\n",
    "X_train_mfcc, X_test_mfcc, y_train_mfcc, y_test_mfcc = train_test_split(X_mfcc, y_mfcc, test_size=0.2, random_state=12)\n",
    "\n",
    "# Load and split data for Mel Spectrogram\n",
    "X_mel, y_mel = load_features(\"mel_spectrogram\")\n",
    "X_train_mel, X_test_mel, y_train_mel, y_test_mel = train_test_split(X_mel, y_mel, test_size=0.2, random_state=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating models for MFCC features:\n",
      "\n",
      "KNN Model Accuracy: 0.26\n",
      "Random Forest Model Accuracy: 0.39\n",
      "SVM Model Accuracy: 0.33\n",
      "\n",
      "Evaluating models for Mel Spectrogram features:\n",
      "\n",
      "KNN Model Accuracy: 0.09\n",
      "Random Forest Model Accuracy: 0.34\n",
      "SVM Model Accuracy: 0.14\n"
     ]
    }
   ],
   "source": [
    "# ML Models \n",
    "\n",
    "# Initialize models\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=12)\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Model List\n",
    "models = {\"KNN\": knn, \"Random Forest\": rf, \"SVM\": svm}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, feature_type):\n",
    "    print(f\"\\nEvaluating models for {feature_type} features:\\n\")\n",
    "    for name, model in models.items():\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluation\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"{name} Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# MFCC\n",
    "train_and_evaluate(X_train_mfcc, X_test_mfcc, y_train_mfcc, y_test_mfcc, \"MFCC\")\n",
    "\n",
    "# Mel Spectrogram\n",
    "train_and_evaluate(X_train_mel, X_test_mel, y_train_mel, y_test_mel, \"Mel Spectrogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "\n",
    "        dummy_input = torch.zeros(1, 1, 128, 216)  # (batch_size, channels, height, width)\n",
    "        out = self.pool(self.relu(self.conv1(dummy_input)))\n",
    "        out = self.pool(self.relu(self.conv2(out)))\n",
    "        self.flattened_size = out.numel()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, metadata, feature_type, feature_dir, num_classes):\n",
    "        self.metadata = metadata\n",
    "        self.feature_type = feature_type\n",
    "        self.feature_dir = feature_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.label_map = {label: idx for idx, label in enumerate(metadata['category'].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        file_name = row[\"filename\"].split(\".\")[0]\n",
    "        feature_path = f\"{self.feature_dir}/{file_name}_{self.feature_type}.npy\"\n",
    "        features = np.load(feature_path)\n",
    "\n",
    "        # Normalize features and add channel dimension\n",
    "        features = (features - np.mean(features)) / np.std(features)\n",
    "        features = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        label = self.label_map[row[\"category\"]]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=50, rnn_hidden_size=128, num_rnn_layers=2):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "\n",
    "        # Compute input size for RNN\n",
    "        dummy_input = torch.randn(1, input_channels, 128, 216)  # (batch, channels, height, width)\n",
    "        self.rnn_input_size = self._get_rnn_input_size(dummy_input)\n",
    "\n",
    "        # Recurrent layers\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.rnn_input_size,\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            num_layers=num_rnn_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(rnn_hidden_size * 2, num_classes)  # *2 for bidirectional\n",
    "\n",
    "    def _get_rnn_input_size(self, x):\n",
    "        \"\"\"Pass dummy data through CNN layers to calculate flattened output size.\"\"\"\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        _, _, height, width = x.shape\n",
    "        return height * 32\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through CNN layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Flatten for RNN input\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        x = x.view(batch_size, width, channels * height)\n",
    "\n",
    "        # Pass through RNN\n",
    "        x, _ = self.rnn(x)\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Pass through fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, device, num_epochs=25):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SimpleCNN...\n",
      "Epoch [1/100], Loss: 4.1750\n",
      "Epoch [2/100], Loss: 3.7834\n",
      "Epoch [3/100], Loss: 3.6422\n",
      "Epoch [4/100], Loss: 3.4423\n",
      "Epoch [5/100], Loss: 3.3508\n",
      "Epoch [6/100], Loss: 3.1063\n",
      "Epoch [7/100], Loss: 3.0427\n",
      "Epoch [8/100], Loss: 2.9605\n",
      "Epoch [9/100], Loss: 2.8171\n",
      "Epoch [10/100], Loss: 2.6855\n",
      "Epoch [11/100], Loss: 2.6092\n",
      "Epoch [12/100], Loss: 2.5533\n",
      "Epoch [13/100], Loss: 2.5229\n",
      "Epoch [14/100], Loss: 2.4346\n",
      "Epoch [15/100], Loss: 2.2952\n",
      "Epoch [16/100], Loss: 2.2480\n",
      "Epoch [17/100], Loss: 2.1982\n",
      "Epoch [18/100], Loss: 2.1823\n",
      "Epoch [19/100], Loss: 2.1255\n",
      "Epoch [20/100], Loss: 2.0263\n",
      "Epoch [21/100], Loss: 2.0018\n",
      "Epoch [22/100], Loss: 1.9456\n",
      "Epoch [23/100], Loss: 1.9355\n",
      "Epoch [24/100], Loss: 1.9034\n",
      "Epoch [25/100], Loss: 1.8603\n",
      "Epoch [26/100], Loss: 1.8246\n",
      "Epoch [27/100], Loss: 1.8208\n",
      "Epoch [28/100], Loss: 1.7674\n",
      "Epoch [29/100], Loss: 1.7669\n",
      "Epoch [30/100], Loss: 1.6801\n",
      "Epoch [31/100], Loss: 1.6958\n",
      "Epoch [32/100], Loss: 1.6524\n",
      "Epoch [33/100], Loss: 1.6088\n",
      "Epoch [34/100], Loss: 1.5915\n",
      "Epoch [35/100], Loss: 1.6137\n",
      "Epoch [36/100], Loss: 1.5853\n",
      "Epoch [37/100], Loss: 1.5527\n",
      "Epoch [38/100], Loss: 1.5649\n",
      "Epoch [39/100], Loss: 1.5994\n",
      "Epoch [40/100], Loss: 1.4600\n",
      "Epoch [41/100], Loss: 1.5146\n",
      "Epoch [42/100], Loss: 1.4474\n",
      "Epoch [43/100], Loss: 1.4397\n",
      "Epoch [44/100], Loss: 1.4158\n",
      "Epoch [45/100], Loss: 1.4091\n",
      "Epoch [46/100], Loss: 1.3466\n",
      "Epoch [47/100], Loss: 1.4143\n",
      "Epoch [48/100], Loss: 1.4432\n",
      "Epoch [49/100], Loss: 1.3958\n",
      "Epoch [50/100], Loss: 1.3548\n",
      "Epoch [51/100], Loss: 1.3842\n",
      "Epoch [52/100], Loss: 1.4103\n",
      "Epoch [53/100], Loss: 1.3487\n",
      "Epoch [54/100], Loss: 1.3378\n",
      "Epoch [55/100], Loss: 1.2902\n",
      "Epoch [56/100], Loss: 1.3012\n",
      "Epoch [57/100], Loss: 1.2584\n",
      "Epoch [58/100], Loss: 1.3046\n",
      "Epoch [59/100], Loss: 1.2422\n",
      "Epoch [60/100], Loss: 1.2425\n",
      "Epoch [61/100], Loss: 1.2735\n",
      "Epoch [62/100], Loss: 1.2719\n",
      "Epoch [63/100], Loss: 1.2173\n",
      "Epoch [64/100], Loss: 1.2490\n",
      "Epoch [65/100], Loss: 1.2404\n",
      "Epoch [66/100], Loss: 1.2176\n",
      "Epoch [67/100], Loss: 1.2437\n",
      "Epoch [68/100], Loss: 1.1835\n",
      "Epoch [69/100], Loss: 1.1689\n",
      "Epoch [70/100], Loss: 1.2749\n",
      "Epoch [71/100], Loss: 1.2242\n",
      "Epoch [72/100], Loss: 1.1791\n",
      "Epoch [73/100], Loss: 1.1385\n",
      "Epoch [74/100], Loss: 1.1410\n",
      "Epoch [75/100], Loss: 1.1914\n",
      "Epoch [76/100], Loss: 1.1354\n",
      "Epoch [77/100], Loss: 1.1382\n",
      "Epoch [78/100], Loss: 1.1577\n",
      "Epoch [79/100], Loss: 1.1163\n",
      "Epoch [80/100], Loss: 1.1661\n",
      "Epoch [81/100], Loss: 1.1293\n",
      "Epoch [82/100], Loss: 1.1211\n",
      "Epoch [83/100], Loss: 1.1707\n",
      "Epoch [84/100], Loss: 1.1620\n",
      "Epoch [85/100], Loss: 1.0686\n",
      "Epoch [86/100], Loss: 1.1512\n",
      "Epoch [87/100], Loss: 1.0882\n",
      "Epoch [88/100], Loss: 1.0917\n",
      "Epoch [89/100], Loss: 1.0649\n",
      "Epoch [90/100], Loss: 1.0703\n",
      "Epoch [91/100], Loss: 1.0523\n",
      "Epoch [92/100], Loss: 1.0529\n",
      "Epoch [93/100], Loss: 1.0296\n",
      "Epoch [94/100], Loss: 1.0308\n",
      "Epoch [95/100], Loss: 1.0807\n",
      "Epoch [96/100], Loss: 1.0221\n",
      "Epoch [97/100], Loss: 1.0326\n",
      "Epoch [98/100], Loss: 1.0535\n",
      "Epoch [99/100], Loss: 1.0569\n",
      "Epoch [100/100], Loss: 1.0288\n",
      "Training complete!\n",
      "SimpleCNN Test Accuracy: 32.00%\n",
      "\n",
      "Training CRNN...\n",
      "Epoch [1/100], Loss: 3.9000\n",
      "Epoch [2/100], Loss: 3.6869\n",
      "Epoch [3/100], Loss: 3.2592\n",
      "Epoch [4/100], Loss: 2.9136\n",
      "Epoch [5/100], Loss: 2.7062\n",
      "Epoch [6/100], Loss: 2.5318\n",
      "Epoch [7/100], Loss: 2.4351\n",
      "Epoch [8/100], Loss: 2.3726\n",
      "Epoch [9/100], Loss: 2.2442\n",
      "Epoch [10/100], Loss: 2.1895\n",
      "Epoch [11/100], Loss: 2.0945\n",
      "Epoch [12/100], Loss: 2.0680\n",
      "Epoch [13/100], Loss: 1.9665\n",
      "Epoch [14/100], Loss: 1.9086\n",
      "Epoch [15/100], Loss: 1.8103\n",
      "Epoch [16/100], Loss: 1.7614\n",
      "Epoch [17/100], Loss: 1.7226\n",
      "Epoch [18/100], Loss: 1.6781\n",
      "Epoch [19/100], Loss: 1.6430\n",
      "Epoch [20/100], Loss: 1.5849\n",
      "Epoch [21/100], Loss: 1.5541\n",
      "Epoch [22/100], Loss: 1.4659\n",
      "Epoch [23/100], Loss: 1.4539\n",
      "Epoch [24/100], Loss: 1.3517\n",
      "Epoch [25/100], Loss: 1.2738\n",
      "Epoch [26/100], Loss: 1.2732\n",
      "Epoch [27/100], Loss: 1.2401\n",
      "Epoch [28/100], Loss: 1.1731\n",
      "Epoch [29/100], Loss: 1.1175\n",
      "Epoch [30/100], Loss: 1.1035\n",
      "Epoch [31/100], Loss: 1.0840\n",
      "Epoch [32/100], Loss: 1.0211\n",
      "Epoch [33/100], Loss: 1.0170\n",
      "Epoch [34/100], Loss: 0.9751\n",
      "Epoch [35/100], Loss: 0.9433\n",
      "Epoch [36/100], Loss: 0.8975\n",
      "Epoch [37/100], Loss: 0.8776\n",
      "Epoch [38/100], Loss: 0.8685\n",
      "Epoch [39/100], Loss: 0.8370\n",
      "Epoch [40/100], Loss: 0.8229\n",
      "Epoch [41/100], Loss: 0.7816\n",
      "Epoch [42/100], Loss: 0.7699\n",
      "Epoch [43/100], Loss: 0.7141\n",
      "Epoch [44/100], Loss: 0.6758\n",
      "Epoch [45/100], Loss: 0.6686\n",
      "Epoch [46/100], Loss: 0.6320\n",
      "Epoch [47/100], Loss: 0.6191\n",
      "Epoch [48/100], Loss: 0.6494\n",
      "Epoch [49/100], Loss: 0.5789\n",
      "Epoch [50/100], Loss: 0.6071\n",
      "Epoch [51/100], Loss: 0.6315\n",
      "Epoch [52/100], Loss: 0.5776\n",
      "Epoch [53/100], Loss: 0.5707\n",
      "Epoch [54/100], Loss: 0.5771\n",
      "Epoch [55/100], Loss: 0.5319\n",
      "Epoch [56/100], Loss: 0.5018\n",
      "Epoch [57/100], Loss: 0.4825\n",
      "Epoch [58/100], Loss: 0.4724\n",
      "Epoch [59/100], Loss: 0.5026\n",
      "Epoch [60/100], Loss: 0.4572\n",
      "Epoch [61/100], Loss: 0.4551\n",
      "Epoch [62/100], Loss: 0.4280\n",
      "Epoch [63/100], Loss: 0.4017\n",
      "Epoch [64/100], Loss: 0.4062\n",
      "Epoch [65/100], Loss: 0.3971\n",
      "Epoch [66/100], Loss: 0.4107\n",
      "Epoch [67/100], Loss: 0.4000\n",
      "Epoch [68/100], Loss: 0.4056\n",
      "Epoch [69/100], Loss: 0.4016\n",
      "Epoch [70/100], Loss: 0.3811\n",
      "Epoch [71/100], Loss: 0.3561\n",
      "Epoch [72/100], Loss: 0.3435\n",
      "Epoch [73/100], Loss: 0.3702\n",
      "Epoch [74/100], Loss: 0.3498\n",
      "Epoch [75/100], Loss: 0.3120\n",
      "Epoch [76/100], Loss: 0.3174\n",
      "Epoch [77/100], Loss: 0.3164\n",
      "Epoch [78/100], Loss: 0.3082\n",
      "Epoch [79/100], Loss: 0.2822\n",
      "Epoch [80/100], Loss: 0.2997\n",
      "Epoch [81/100], Loss: 0.3156\n",
      "Epoch [82/100], Loss: 0.3004\n",
      "Epoch [83/100], Loss: 0.2800\n",
      "Epoch [84/100], Loss: 0.2970\n",
      "Epoch [85/100], Loss: 0.2674\n",
      "Epoch [86/100], Loss: 0.2578\n",
      "Epoch [87/100], Loss: 0.2903\n",
      "Epoch [88/100], Loss: 0.2783\n",
      "Epoch [89/100], Loss: 0.2763\n",
      "Epoch [90/100], Loss: 0.2558\n",
      "Epoch [91/100], Loss: 0.2470\n",
      "Epoch [92/100], Loss: 0.2768\n",
      "Epoch [93/100], Loss: 0.2635\n",
      "Epoch [94/100], Loss: 0.2624\n",
      "Epoch [95/100], Loss: 0.2520\n",
      "Epoch [96/100], Loss: 0.2477\n",
      "Epoch [97/100], Loss: 0.2258\n",
      "Epoch [98/100], Loss: 0.2211\n",
      "Epoch [99/100], Loss: 0.2253\n",
      "Epoch [100/100], Loss: 0.2205\n",
      "Training complete!\n",
      "CRNN Test Accuracy: 34.75%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    feature_type = \"mel_spectrogram\"\n",
    "    feature_dir = f\"processed_data/{feature_type}\"\n",
    "    metadata = pd.read_csv(\"dataset/esc50.csv\")\n",
    "    num_classes = len(metadata[\"category\"].unique())\n",
    "    num_epochs = 100\n",
    "    # Create dataset and dataloaders\n",
    "    dataset = AudioDataset(metadata, feature_type, feature_dir, num_classes)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CRNN(input_channels=1, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train SimpleCNN\n",
    "    print(\"Training SimpleCNN...\")\n",
    "    cnn_model = SimpleCNN(num_classes=num_classes).to(device)\n",
    "    train_model(cnn_model, train_loader, test_loader, device, num_epochs)\n",
    "\n",
    "    # Evaluate SimpleCNN\n",
    "    cnn_accuracy = evaluate_model(cnn_model, test_loader, device)\n",
    "    print(f\"SimpleCNN Test Accuracy: {cnn_accuracy:.2f}%\")\n",
    "\n",
    "    # Train CRNN\n",
    "    print(\"\\nTraining CRNN...\")\n",
    "    crnn_model = CRNN(input_channels=1, num_classes=num_classes).to(device)\n",
    "    train_model(crnn_model, train_loader, test_loader, device, num_epochs)\n",
    "\n",
    "    # Evaluate CRNN\n",
    "    crnn_accuracy = evaluate_model(crnn_model, test_loader, device)\n",
    "    print(f\"CRNN Test Accuracy: {crnn_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jimmy_coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
