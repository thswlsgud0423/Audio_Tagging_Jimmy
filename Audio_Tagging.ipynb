{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# file_path\n",
    "audio_path = Path(\"dataset\") / \"audio\"\n",
    "csv_path = Path(\"dataset\") / \"esc50.csv\"\n",
    "\n",
    "metadata = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsr = 22050\\nduration = 5 # ESC50 has 5 sec duration\\nn_mfcc = 13\\nn_mels = 128\\n\\n# Output\\nmfcc_dir = Path(\"processed_data\") / \"mfcc\"\\nmel_dir = Path(\"processed_data\") / \"mel_spectrogram\"\\nmfcc_dir.mkdir(parents=True, exist_ok=True)\\nmel_dir.mkdir(parents=True, exist_ok=True)\\n\\n# Function to save MFCC and Mel Spectrogram\\ndef save_features(audio_path, filename, sr=22050):\\n    # Load audio\\n    y, _ = librosa.load(audio_path, sr=sr, duration=duration)\\n\\n    # Compute MFCC and Mel_spectrogram\\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\\n    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\\n\\n    # Save as .npy\\n    np.save(f\"processed_data/mfcc/{filename}_mfcc.npy\", mfcc)\\n    np.save(f\"processed_data/mel_spectrogram/{filename}_mel_spectrogram.npy\", mel_spectrogram)\\n\\n# Save features\\nfor _, row in metadata.iterrows():\\n    audio_file = f\"dataset/audio/{row[\\'filename\\']}\"\\n    filename = row[\"filename\"].split(\".\")[0]\\n    save_features(audio_file, filename) '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sr = 22050\n",
    "duration = 5 # ESC50 has 5 sec duration\n",
    "n_mfcc = 13\n",
    "n_mels = 128\n",
    "\n",
    "# Output\n",
    "mfcc_dir = Path(\"processed_data\") / \"mfcc\"\n",
    "mel_dir = Path(\"processed_data\") / \"mel_spectrogram\"\n",
    "mfcc_dir.mkdir(parents=True, exist_ok=True)\n",
    "mel_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Function to save MFCC and Mel Spectrogram\n",
    "def save_features(audio_path, filename, sr=22050):\n",
    "    # Load audio\n",
    "    y, _ = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "\n",
    "    # Compute MFCC and Mel_spectrogram\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "\n",
    "    # Save as .npy\n",
    "    np.save(f\"processed_data/mfcc/{filename}_mfcc.npy\", mfcc)\n",
    "    np.save(f\"processed_data/mel_spectrogram/{filename}_mel_spectrogram.npy\", mel_spectrogram)\n",
    "\n",
    "# Save features\n",
    "for _, row in metadata.iterrows():\n",
    "    audio_file = f\"dataset/audio/{row['filename']}\"\n",
    "    filename = row[\"filename\"].split(\".\")[0]\n",
    "    save_features(audio_file, filename) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata = pd.read_csv(\"dataset/esc50.csv\")\n",
    "\n",
    "# Function to load features and flatten them\n",
    "def load_features(feature_type=\"mfcc\"):\n",
    "    feature_dir = Path(f\"processed_data/{feature_type}\")\n",
    "    X, y = [], []\n",
    "    for _, row in metadata.iterrows():\n",
    "        class_label = row[\"category\"]\n",
    "        file_name = row[\"filename\"].split(\".\")[0]\n",
    "        \n",
    "        # Load .npy file\n",
    "        feature_path = feature_dir / f\"{file_name}_{feature_type}.npy\"\n",
    "        features = np.load(feature_path)\n",
    "        \n",
    "        # Flatten the features to 1D for simple models\n",
    "        X.append(features.flatten())\n",
    "        y.append(class_label)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load and split data for MFCC\n",
    "X_mfcc, y_mfcc = load_features(\"mfcc\")\n",
    "X_train_mfcc, X_test_mfcc, y_train_mfcc, y_test_mfcc = train_test_split(X_mfcc, y_mfcc, test_size=0.2, random_state=12)\n",
    "\n",
    "# Load and split data for Mel Spectrogram\n",
    "X_mel, y_mel = load_features(\"mel_spectrogram\")\n",
    "X_train_mel, X_test_mel, y_train_mel, y_test_mel = train_test_split(X_mel, y_mel, test_size=0.2, random_state=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize models\\nknn = KNeighborsClassifier(n_neighbors=4)\\nrf = RandomForestClassifier(n_estimators=100, random_state=12)\\nsvm = SVC(kernel=\"linear\")\\n\\n# Model List\\nmodels = {\"KNN\": knn, \"Random Forest\": rf, \"SVM\": svm}\\n\\n# Function to train and evaluate models\\ndef train_and_evaluate(X_train, X_test, y_train, y_test, feature_type):\\n    print(f\"\\nEvaluating models for {feature_type} features:\\n\")\\n    for name, model in models.items():\\n        # Train the model\\n        model.fit(X_train, y_train)\\n        \\n        y_pred = model.predict(X_test)\\n        \\n        # Evaluation\\n        accuracy = accuracy_score(y_test, y_pred)\\n        print(f\"{name} Model Accuracy: {accuracy:.2f}\")\\n\\n# MFCC\\ntrain_and_evaluate(X_train_mfcc, X_test_mfcc, y_train_mfcc, y_test_mfcc, \"MFCC\")\\n\\n# Mel Spectrogram\\ntrain_and_evaluate(X_train_mel, X_test_mel, y_train_mel, y_test_mel, \"Mel Spectrogram\") '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ML Models \n",
    "\"\"\"\n",
    "# Initialize models\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=12)\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Model List\n",
    "models = {\"KNN\": knn, \"Random Forest\": rf, \"SVM\": svm}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, feature_type):\n",
    "    print(f\"\\nEvaluating models for {feature_type} features:\\n\")\n",
    "    for name, model in models.items():\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluation\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"{name} Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# MFCC\n",
    "train_and_evaluate(X_train_mfcc, X_test_mfcc, y_train_mfcc, y_test_mfcc, \"MFCC\")\n",
    "\n",
    "# Mel Spectrogram\n",
    "train_and_evaluate(X_train_mel, X_test_mel, y_train_mel, y_test_mel, \"Mel Spectrogram\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        dummy_input = torch.zeros(1, 1, 128, 216)  # (batch_size, channels, height, width)\n",
    "        out = self.pool(self.relu(self.conv1(dummy_input)))\n",
    "        out = self.pool(self.relu(self.conv2(out)))\n",
    "        self.flattened_size = out.numel()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, metadata, feature_type, feature_dir, num_classes):\n",
    "        self.metadata = metadata\n",
    "        self.feature_type = feature_type\n",
    "        self.feature_dir = feature_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.label_map = {label: idx for idx, label in enumerate(metadata['category'].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        file_name = row[\"filename\"].split(\".\")[0]\n",
    "        feature_path = f\"{self.feature_dir}/{file_name}_{self.feature_type}.npy\"\n",
    "        features = np.load(feature_path)\n",
    "\n",
    "        # Normalize features and add channel dimension\n",
    "        features = (features - np.mean(features)) / np.std(features)\n",
    "        features = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        label = self.label_map[row[\"category\"]]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, device, num_epochs=25):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the predicted class (index of max logit)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "        epoch_accuracy = (correct_predictions / total_predictions) * 100  # in percentage\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCRNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, img_height=128, img_width=256, num_classes=50,\n",
    "                 map_to_seq_hidden=128, rnn_hidden_size=256, num_rnn_layers=3, dropout=0.3):\n",
    "        super(ImprovedCRNN, self).__init__()\n",
    "        \n",
    "        # CNN backbone\n",
    "        self.cnn, (output_channels, output_height, output_width) = self._cnn_backbone(\n",
    "            input_channels, img_height, img_width\n",
    "        )\n",
    "        \n",
    "        # Map CNN output to sequence\n",
    "        self.map_to_seq = nn.Linear(output_channels * output_height, map_to_seq_hidden)\n",
    "        \n",
    "        # Recurrent layers\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            map_to_seq_hidden,\n",
    "            rnn_hidden_size,\n",
    "            num_layers=num_rnn_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(rnn_hidden_size * 2, num_classes)\n",
    "\n",
    "    def _cnn_backbone(self, input_channels, img_height, img_width):\n",
    "        \n",
    "        cnn = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # Halves height and width\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # Halves height and width\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1)),  # Halves height, keeps width\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1)),  # Halves height, keeps width\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # Calculate output shape\n",
    "        final_height = img_height // (2 * 2 * 2 * 2)  # Height halved 4 times\n",
    "        final_width = img_width // (2 * 2)  # Width halved 2 times\n",
    "        output_shape = (512, final_height, final_width)\n",
    "\n",
    "        return cnn, output_shape\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.cnn(x)\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        x = x.view(batch_size, channels * height, width).permute(2, 0, 1)\n",
    "        x = self.map_to_seq(x)\n",
    "        x, _ = self.rnn1(x)\n",
    "        x = self.fc(x[-1])\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CRNN...\n",
      "Epoch [1/10], Loss: 3.6861, Accuracy: 5.38%\n",
      "Epoch [2/10], Loss: 3.2596, Accuracy: 10.56%\n",
      "Epoch [3/10], Loss: 3.0771, Accuracy: 13.00%\n",
      "Epoch [4/10], Loss: 2.9714, Accuracy: 16.19%\n",
      "Epoch [5/10], Loss: 2.8264, Accuracy: 19.19%\n",
      "Epoch [6/10], Loss: 2.7035, Accuracy: 20.94%\n",
      "Epoch [7/10], Loss: 2.6218, Accuracy: 23.56%\n",
      "Epoch [8/10], Loss: 2.5252, Accuracy: 25.12%\n",
      "Epoch [9/10], Loss: 2.4960, Accuracy: 25.06%\n",
      "Epoch [10/10], Loss: 2.4147, Accuracy: 27.81%\n",
      "Training complete!\n",
      "CRNN Test Accuracy: 29.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    feature_type = \"mel_spectrogram\"\n",
    "    feature_dir = f\"processed_data/{feature_type}\"\n",
    "    metadata = pd.read_csv(\"dataset/esc50.csv\")\n",
    "    num_classes = len(metadata[\"category\"].unique())\n",
    "    num_epochs = 10\n",
    "\n",
    "    # Create dataset and dataloaders\n",
    "    dataset = AudioDataset(metadata, feature_type, feature_dir, num_classes)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ImprovedCRNN(input_channels=1, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    # Train SimpleCNN\n",
    "    print(\"Training SimpleCNN...\")\n",
    "    cnn_model = SimpleCNN(num_classes=num_classes).to(device)\n",
    "    train_model(cnn_model, train_loader, test_loader, device, num_epochs)\n",
    "\n",
    "    # Evaluate SimpleCNN\n",
    "    cnn_accuracy = evaluate_model(cnn_model, test_loader, device)\n",
    "    print(f\"SimpleCNN Test Accuracy: {cnn_accuracy:.2f}%\")\n",
    "    \"\"\"\n",
    "    # Train CRNN\n",
    "    print(\"\\nTraining CRNN...\")\n",
    "    crnn_model = ImprovedCRNN(input_channels=1, num_classes=num_classes).to(device)\n",
    "    train_model(crnn_model, train_loader, test_loader, device, num_epochs)\n",
    "\n",
    "    # Evaluate CRNN\n",
    "    crnn_accuracy = evaluate_model(crnn_model, test_loader, device)\n",
    "    print(f\"CRNN Test Accuracy: {crnn_accuracy:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities_to_text(probabilities, class_labels, top_n=3):\n",
    "    # Ensure probabilities are a 2D tensor\n",
    "    if len(probabilities.shape) != 2:\n",
    "        raise ValueError(\"Probabilities should be a 2D tensor (batch_size, num_classes).\")\n",
    "    \n",
    "    # Get the top N indices for the first item in the batch\n",
    "    top_indices = probabilities[0].argsort(descending=True)[:top_n]\n",
    "\n",
    "    # Ensure indices don't exceed class_labels length\n",
    "    assert len(class_labels) >= probabilities.size(1), (\n",
    "        \"Number of class labels does not match the number of classes in the model output.\"\n",
    "    )\n",
    "    \n",
    "    # Construct the description\n",
    "    description = \"The audio likely contains \"\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        prob = probabilities[0][idx].item() * 100\n",
    "        description += f\"{class_labels[idx]} ({prob:.1f}%)\"\n",
    "        if i < top_n - 1:\n",
    "            description += \", \"\n",
    "        else:\n",
    "            description += \".\"\n",
    "    return description\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The audio likely contains shuffling (2.2%), printer (2.1%), whistling (2.1%).\n"
     ]
    }
   ],
   "source": [
    "# Generate description with top predictions\n",
    "description = probabilities_to_text(probabilities, class_labels)\n",
    "print(description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2LMHeadModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load GPT-2 model and tokenizer\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m gpt2_model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      4\u001b[0m gpt2_tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      5\u001b[0m gpt2_model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GPT2LMHeadModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "gpt2_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Function to generate description from probabilities\n",
    "# Function to generate description from probabilities\n",
    "def probabilities_to_text(probabilities, class_labels, top_n=3):\n",
    "    # Ensure probabilities is a 2D tensor with shape (batch_size, num_classes)\n",
    "    # Select the first (and only) sample from the batch (index 0)\n",
    "    probs = probabilities[0]  # This is the probability vector for the first sample\n",
    "\n",
    "    # Sort probabilities and get top_n\n",
    "    top_indices = torch.argsort(probs, descending=True)[:top_n]\n",
    "    description = \"Based on the classification, the audio likely contains: \"\n",
    "\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        prob = probs[idx].item() * 100  # Use the selected probability\n",
    "        description += f\"{class_labels[idx]} ({prob:.1f}%)\"\n",
    "        if i < top_n - 1:\n",
    "            description += \", \"\n",
    "    \n",
    "    return description\n",
    "\n",
    "\n",
    "# Function to generate text interpretation from GPT-2 based on classification results\n",
    "def generate_text_with_gpt2(classification_description):\n",
    "    prompt = f\"\"\"\n",
    "Here is the analysis from an audio classification model:\n",
    "The audio contains sounds like dog barking (30%) and bird chirping (10%).\n",
    "\n",
    "The classification suggests that the audio likely contains the following sounds:\n",
    "1. Rain - 2.0%\n",
    "2. Footsteps - 2.0%\n",
    "3. Cat - 2.0%\n",
    "\n",
    "Please provide:\n",
    "1. A detailed explanation of what might be happening in the audio based on the classification results above.\n",
    "2. Specific recommendations or actions to take if this audio is being monitored in an environmental monitoring system. For example, what should be done if the audio suggests human presence, or an anomaly in environmental conditions.\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "    # Encode the input prompt\n",
    "    inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model.generate(inputs['input_ids'], max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    return gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example audio input (replace this with actual audio input processing)\n",
    "dummy_audio_input = torch.randn(1, 1, 128, 216)  # Example input (batch_size, channels, height, width)\n",
    "\n",
    "# Example class labels (replace with actual class labels for your dataset)\n",
    "class_labels = class_labels = [\n",
    "    \"dog\", \"cat\", \"car_horn\", \"chirping_birds\", \"airplane\", \"rain\",\n",
    "    \"siren\", \"engine\", \"crying_baby\", \"crow\", \n",
    "    \"footsteps\", \"laughing\", \"brushing_teeth\", \"clapping\", \"keyboard_typing\",\n",
    "    \"coughing\", \"sneezing\", \"knocking\", \"phone_ringing\", \"door_wood_knock\",\n",
    "    \"washing_machine\", \"vacuum_cleaner\", \"glass_breaking\", \"fireworks\", \"door_wood_creak\",\n",
    "    \"helicopter\", \"chainsaw\", \"mouse_click\", \"cup_filling\", \"printer\", \n",
    "    \"thunderstorm\", \"wind\", \"dripping_water\", \"toilet_flush\", \"drill\",\n",
    "    \"camera\", \"camera_shutter\", \"snoring\", \"speech\", \"harmonica\",\n",
    "    \"guitar\", \"flute\", \"drums\", \"violin\", \"double_bass\", \n",
    "    \"fart\", \"belch\", \"whistling\", \"shouting\", \"shuffling\"\n",
    "]  # Add all 50 labels as needed\n",
    "\n",
    "# Initialize the CRNN model and run a forward pass\n",
    "crnn_model = CRNN(num_classes=len(class_labels))\n",
    "output = crnn_model(dummy_audio_input)\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = F.softmax(output, dim=1)\n",
    "\n",
    "# Get a text description of the classification results\n",
    "classification_description = probabilities_to_text(probabilities, class_labels)\n",
    "\n",
    "# Generate a text interpretation from GPT-2\n",
    "generated_text = generate_text_with_gpt2(classification_description)\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jimmy_coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
