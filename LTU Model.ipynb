{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTU Model\n",
    "https://github.com/YuanGongND/ltu?tab=readme-ov-file#for-ltu-as-openasqa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I will make a simpler model.\n",
    "\n",
    "1. Audio Input:\n",
    "    Load an audio file and preprocess it to extract Mel spectrograms.\n",
    "2. CRNN Processing:\n",
    "    Predict the class probabilities using the CRNN model.\n",
    "3. Natural Language Conversion:\n",
    "    Translate probabilities into human-readable text.\n",
    "4. LLM Interaction:\n",
    "    Use the text as input for the LLM.\n",
    "    Query the LLM for reasoning, explanation, or decision-making tasks.\n",
    "5. Output:\n",
    "    Provide explanations, classifications, or environmental scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Using Class Probabilities\n",
    "Instead of a single class label, the LLM can use the full probability distribution to reason about uncertainty. For instance:\n",
    "\n",
    "CRNN Output: [0.1, 0.3, ..., 0.05]\n",
    "Text Input to LLM: \"The audio likely contains the sound of a dog barking (30%) but may also include a bird chirping (10%).\"\n",
    "\n",
    "\n",
    "--> Simpler and can simply used the outcome from the outcome \n",
    "--> Maybe it can induce the situation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# file_path\n",
    "audio_path = Path(\"dataset\") / \"audio\"\n",
    "csv_path = Path(\"dataset\") / \"esc50.csv\"\n",
    "\n",
    "metadata = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata = pd.read_csv(\"dataset/esc50.csv\")\n",
    "\n",
    "# Function to load features and flatten them\n",
    "def load_features(feature_type=\"mfcc\"):\n",
    "    feature_dir = Path(f\"processed_data/{feature_type}\")\n",
    "    X, y = [], []\n",
    "    for _, row in metadata.iterrows():\n",
    "        class_label = row[\"category\"]\n",
    "        file_name = row[\"filename\"].split(\".\")[0]\n",
    "        \n",
    "        # Load .npy file\n",
    "        feature_path = feature_dir / f\"{file_name}_{feature_type}.npy\"\n",
    "        features = np.load(feature_path)\n",
    "        \n",
    "        # Flatten the features to 1D for simple models\n",
    "        X.append(features.flatten())\n",
    "        y.append(class_label)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load and split data for MFCC\n",
    "X_mfcc, y_mfcc = load_features(\"mfcc\")\n",
    "X_train_mfcc, X_test_mfcc, y_train_mfcc, y_test_mfcc = train_test_split(X_mfcc, y_mfcc, test_size=0.2, random_state=12)\n",
    "\n",
    "# Load and split data for Mel Spectrogram\n",
    "X_mel, y_mel = load_features(\"mel_spectrogram\")\n",
    "X_train_mel, X_test_mel, y_train_mel, y_test_mel = train_test_split(X_mel, y_mel, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, metadata, feature_type, feature_dir, num_classes):\n",
    "        self.metadata = metadata\n",
    "        self.feature_type = feature_type\n",
    "        self.feature_dir = feature_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.label_map = {label: idx for idx, label in enumerate(metadata['category'].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        file_name = row[\"filename\"].split(\".\")[0]\n",
    "        feature_path = f\"{self.feature_dir}/{file_name}_{self.feature_type}.npy\"\n",
    "        features = np.load(feature_path)\n",
    "\n",
    "        # Normalize features and add channel dimension\n",
    "        features = (features - np.mean(features)) / np.std(features)\n",
    "        features = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        label = self.label_map[row[\"category\"]]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # RNN\n",
    "        self.rnn_hidden_size = 128\n",
    "        self.rnn = nn.LSTM(128, self.rnn_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(self.rnn_hidden_size * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.bn1(torch.relu(self.conv1(x))))\n",
    "        x = self.pool(self.bn2(torch.relu(self.conv2(x))))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Prepare for RNN\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  # Rearrange to (batch, seq_len, features)\n",
    "        b, seq_len, _, features = x.shape\n",
    "        x = x.view(b, seq_len, -1)\n",
    "\n",
    "        # RNN\n",
    "        x, _ = self.rnn(x)\n",
    "        x = torch.relu(self.fc1(x[:, -1, :]))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, device, num_epochs, criterion, optimizer, scheduler=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_accuracy = (correct / total) * 100\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 3.9118, Accuracy: 1.81%\n",
      "Epoch [2/300], Loss: 3.9064, Accuracy: 3.25%\n",
      "Epoch [3/300], Loss: 3.9014, Accuracy: 2.75%\n",
      "Epoch [4/300], Loss: 3.8955, Accuracy: 4.31%\n",
      "Epoch [5/300], Loss: 3.8874, Accuracy: 4.62%\n",
      "Epoch [6/300], Loss: 3.8793, Accuracy: 4.62%\n",
      "Epoch [7/300], Loss: 3.8665, Accuracy: 5.19%\n",
      "Epoch [8/300], Loss: 3.8484, Accuracy: 6.56%\n",
      "Epoch [9/300], Loss: 3.8295, Accuracy: 6.50%\n",
      "Epoch [10/300], Loss: 3.8085, Accuracy: 6.25%\n",
      "Epoch [11/300], Loss: 3.7905, Accuracy: 7.00%\n",
      "Epoch [12/300], Loss: 3.7735, Accuracy: 7.62%\n",
      "Epoch [13/300], Loss: 3.7599, Accuracy: 8.25%\n",
      "Epoch [14/300], Loss: 3.7442, Accuracy: 8.25%\n",
      "Epoch [15/300], Loss: 3.7333, Accuracy: 8.88%\n",
      "Epoch [16/300], Loss: 3.7177, Accuracy: 10.12%\n",
      "Epoch [17/300], Loss: 3.7049, Accuracy: 10.12%\n",
      "Epoch [18/300], Loss: 3.6903, Accuracy: 10.44%\n",
      "Epoch [19/300], Loss: 3.6721, Accuracy: 10.38%\n",
      "Epoch [20/300], Loss: 3.6567, Accuracy: 11.12%\n",
      "Epoch [21/300], Loss: 3.6379, Accuracy: 10.50%\n",
      "Epoch [22/300], Loss: 3.6180, Accuracy: 11.19%\n",
      "Epoch [23/300], Loss: 3.5947, Accuracy: 10.75%\n",
      "Epoch [24/300], Loss: 3.5801, Accuracy: 10.81%\n",
      "Epoch [25/300], Loss: 3.5603, Accuracy: 10.94%\n",
      "Epoch [26/300], Loss: 3.5381, Accuracy: 11.50%\n",
      "Epoch [27/300], Loss: 3.5170, Accuracy: 12.44%\n",
      "Epoch [28/300], Loss: 3.4933, Accuracy: 12.62%\n",
      "Epoch [29/300], Loss: 3.4663, Accuracy: 13.44%\n",
      "Epoch [30/300], Loss: 3.4447, Accuracy: 14.62%\n",
      "Epoch [31/300], Loss: 3.4220, Accuracy: 14.56%\n",
      "Epoch [32/300], Loss: 3.3950, Accuracy: 15.44%\n",
      "Epoch [33/300], Loss: 3.3749, Accuracy: 16.31%\n",
      "Epoch [34/300], Loss: 3.3448, Accuracy: 17.19%\n",
      "Epoch [35/300], Loss: 3.3228, Accuracy: 16.06%\n",
      "Epoch [36/300], Loss: 3.2960, Accuracy: 17.38%\n",
      "Epoch [37/300], Loss: 3.2766, Accuracy: 19.19%\n",
      "Epoch [38/300], Loss: 3.2421, Accuracy: 19.62%\n",
      "Epoch [39/300], Loss: 3.2154, Accuracy: 20.50%\n",
      "Epoch [40/300], Loss: 3.1747, Accuracy: 20.12%\n",
      "Epoch [41/300], Loss: 3.1480, Accuracy: 20.56%\n",
      "Epoch [42/300], Loss: 3.1090, Accuracy: 22.25%\n",
      "Epoch [43/300], Loss: 3.0723, Accuracy: 23.19%\n",
      "Epoch [44/300], Loss: 3.0421, Accuracy: 24.00%\n",
      "Epoch [45/300], Loss: 3.0015, Accuracy: 25.50%\n",
      "Epoch [46/300], Loss: 2.9667, Accuracy: 25.44%\n",
      "Epoch [47/300], Loss: 2.9270, Accuracy: 26.94%\n",
      "Epoch [48/300], Loss: 2.9023, Accuracy: 27.81%\n",
      "Epoch [49/300], Loss: 2.8685, Accuracy: 28.06%\n",
      "Epoch [50/300], Loss: 2.8446, Accuracy: 29.06%\n",
      "Epoch [51/300], Loss: 2.7933, Accuracy: 30.50%\n",
      "Epoch [52/300], Loss: 2.7741, Accuracy: 29.81%\n",
      "Epoch [53/300], Loss: 2.7251, Accuracy: 32.25%\n",
      "Epoch [54/300], Loss: 2.6954, Accuracy: 31.37%\n",
      "Epoch [55/300], Loss: 2.6734, Accuracy: 33.00%\n",
      "Epoch [56/300], Loss: 2.6507, Accuracy: 32.75%\n",
      "Epoch [57/300], Loss: 2.6114, Accuracy: 34.06%\n",
      "Epoch [58/300], Loss: 2.5838, Accuracy: 33.81%\n",
      "Epoch [59/300], Loss: 2.5746, Accuracy: 35.44%\n",
      "Epoch [60/300], Loss: 2.5268, Accuracy: 35.25%\n",
      "Epoch [61/300], Loss: 2.4916, Accuracy: 36.19%\n",
      "Epoch [62/300], Loss: 2.4699, Accuracy: 38.00%\n",
      "Epoch [63/300], Loss: 2.4375, Accuracy: 38.06%\n",
      "Epoch [64/300], Loss: 2.4296, Accuracy: 38.25%\n",
      "Epoch [65/300], Loss: 2.3956, Accuracy: 39.31%\n",
      "Epoch [66/300], Loss: 2.3997, Accuracy: 38.75%\n",
      "Epoch [67/300], Loss: 2.3641, Accuracy: 39.88%\n",
      "Epoch [68/300], Loss: 2.3274, Accuracy: 40.88%\n",
      "Epoch [69/300], Loss: 2.3142, Accuracy: 41.69%\n",
      "Epoch [70/300], Loss: 2.2784, Accuracy: 42.38%\n",
      "Epoch [71/300], Loss: 2.2734, Accuracy: 42.44%\n",
      "Epoch [72/300], Loss: 2.2505, Accuracy: 43.50%\n",
      "Epoch [73/300], Loss: 2.2289, Accuracy: 43.31%\n",
      "Epoch [74/300], Loss: 2.1929, Accuracy: 46.31%\n",
      "Epoch [75/300], Loss: 2.1849, Accuracy: 45.38%\n",
      "Epoch [76/300], Loss: 2.1464, Accuracy: 46.38%\n",
      "Epoch [77/300], Loss: 2.1246, Accuracy: 47.31%\n",
      "Epoch [78/300], Loss: 2.1150, Accuracy: 46.00%\n",
      "Epoch [79/300], Loss: 2.0975, Accuracy: 46.94%\n",
      "Epoch [80/300], Loss: 2.0813, Accuracy: 47.31%\n",
      "Epoch [81/300], Loss: 2.0306, Accuracy: 48.94%\n",
      "Epoch [82/300], Loss: 2.0403, Accuracy: 47.69%\n",
      "Epoch [83/300], Loss: 2.0230, Accuracy: 49.62%\n",
      "Epoch [84/300], Loss: 1.9984, Accuracy: 50.31%\n",
      "Epoch [85/300], Loss: 1.9747, Accuracy: 49.56%\n",
      "Epoch [86/300], Loss: 1.9536, Accuracy: 51.19%\n",
      "Epoch [87/300], Loss: 1.9521, Accuracy: 51.00%\n",
      "Epoch [88/300], Loss: 1.9285, Accuracy: 52.31%\n",
      "Epoch [89/300], Loss: 1.9096, Accuracy: 51.62%\n",
      "Epoch [90/300], Loss: 1.8858, Accuracy: 52.62%\n",
      "Epoch [91/300], Loss: 1.8644, Accuracy: 53.44%\n",
      "Epoch [92/300], Loss: 1.8650, Accuracy: 54.37%\n",
      "Epoch [93/300], Loss: 1.8363, Accuracy: 53.50%\n",
      "Epoch [94/300], Loss: 1.7980, Accuracy: 56.31%\n",
      "Epoch [95/300], Loss: 1.7796, Accuracy: 55.75%\n",
      "Epoch [96/300], Loss: 1.7716, Accuracy: 56.88%\n",
      "Epoch [97/300], Loss: 1.7737, Accuracy: 57.06%\n",
      "Epoch [98/300], Loss: 1.7422, Accuracy: 59.00%\n",
      "Epoch [99/300], Loss: 1.7503, Accuracy: 56.56%\n",
      "Epoch [100/300], Loss: 1.7044, Accuracy: 59.56%\n",
      "Epoch [101/300], Loss: 1.7022, Accuracy: 58.81%\n",
      "Epoch [102/300], Loss: 1.6637, Accuracy: 59.00%\n",
      "Epoch [103/300], Loss: 1.6561, Accuracy: 59.38%\n",
      "Epoch [104/300], Loss: 1.6738, Accuracy: 58.63%\n",
      "Epoch [105/300], Loss: 1.6573, Accuracy: 59.69%\n",
      "Epoch [106/300], Loss: 1.6359, Accuracy: 60.50%\n",
      "Epoch [107/300], Loss: 1.6100, Accuracy: 61.94%\n",
      "Epoch [108/300], Loss: 1.5845, Accuracy: 62.19%\n",
      "Epoch [109/300], Loss: 1.5912, Accuracy: 61.12%\n",
      "Epoch [110/300], Loss: 1.5655, Accuracy: 62.88%\n",
      "Epoch [111/300], Loss: 1.5487, Accuracy: 62.50%\n",
      "Epoch [112/300], Loss: 1.5320, Accuracy: 63.38%\n",
      "Epoch [113/300], Loss: 1.5091, Accuracy: 63.00%\n",
      "Epoch [114/300], Loss: 1.5217, Accuracy: 61.81%\n",
      "Epoch [115/300], Loss: 1.5103, Accuracy: 63.19%\n",
      "Epoch [116/300], Loss: 1.4746, Accuracy: 64.50%\n",
      "Epoch [117/300], Loss: 1.4579, Accuracy: 65.88%\n",
      "Epoch [118/300], Loss: 1.4342, Accuracy: 66.81%\n",
      "Epoch [119/300], Loss: 1.4349, Accuracy: 66.00%\n",
      "Epoch [120/300], Loss: 1.4052, Accuracy: 66.56%\n",
      "Epoch [121/300], Loss: 1.3956, Accuracy: 66.44%\n",
      "Epoch [122/300], Loss: 1.3966, Accuracy: 67.00%\n",
      "Epoch [123/300], Loss: 1.3806, Accuracy: 66.81%\n",
      "Epoch [124/300], Loss: 1.3737, Accuracy: 67.75%\n",
      "Epoch [125/300], Loss: 1.3536, Accuracy: 68.81%\n",
      "Epoch [126/300], Loss: 1.3361, Accuracy: 68.25%\n",
      "Epoch [127/300], Loss: 1.3282, Accuracy: 68.56%\n",
      "Epoch [128/300], Loss: 1.3239, Accuracy: 68.88%\n",
      "Epoch [129/300], Loss: 1.3191, Accuracy: 69.44%\n",
      "Epoch [130/300], Loss: 1.2904, Accuracy: 70.00%\n",
      "Epoch [131/300], Loss: 1.2887, Accuracy: 70.25%\n",
      "Epoch [132/300], Loss: 1.2583, Accuracy: 71.38%\n",
      "Epoch [133/300], Loss: 1.2478, Accuracy: 71.81%\n",
      "Epoch [134/300], Loss: 1.2499, Accuracy: 71.00%\n",
      "Epoch [135/300], Loss: 1.2267, Accuracy: 72.56%\n",
      "Epoch [136/300], Loss: 1.2209, Accuracy: 73.38%\n",
      "Epoch [137/300], Loss: 1.2105, Accuracy: 72.31%\n",
      "Epoch [138/300], Loss: 1.2032, Accuracy: 72.12%\n",
      "Epoch [139/300], Loss: 1.1642, Accuracy: 75.00%\n",
      "Epoch [140/300], Loss: 1.1679, Accuracy: 74.12%\n",
      "Epoch [141/300], Loss: 1.1653, Accuracy: 73.75%\n",
      "Epoch [142/300], Loss: 1.1440, Accuracy: 73.69%\n",
      "Epoch [143/300], Loss: 1.1432, Accuracy: 74.00%\n",
      "Epoch [144/300], Loss: 1.1280, Accuracy: 74.56%\n",
      "Epoch [145/300], Loss: 1.1085, Accuracy: 76.38%\n",
      "Epoch [146/300], Loss: 1.0921, Accuracy: 76.62%\n",
      "Epoch [147/300], Loss: 1.0941, Accuracy: 75.06%\n",
      "Epoch [148/300], Loss: 1.0571, Accuracy: 78.00%\n",
      "Epoch [149/300], Loss: 1.0588, Accuracy: 77.25%\n",
      "Epoch [150/300], Loss: 1.0624, Accuracy: 76.88%\n",
      "Epoch [151/300], Loss: 1.0374, Accuracy: 77.25%\n",
      "Epoch [152/300], Loss: 1.0449, Accuracy: 76.69%\n",
      "Epoch [153/300], Loss: 1.0094, Accuracy: 79.06%\n",
      "Epoch [154/300], Loss: 1.0038, Accuracy: 78.75%\n",
      "Epoch [155/300], Loss: 0.9972, Accuracy: 78.69%\n",
      "Epoch [156/300], Loss: 0.9862, Accuracy: 79.62%\n",
      "Epoch [157/300], Loss: 0.9751, Accuracy: 79.38%\n",
      "Epoch [158/300], Loss: 0.9763, Accuracy: 78.69%\n",
      "Epoch [159/300], Loss: 0.9742, Accuracy: 79.50%\n",
      "Epoch [160/300], Loss: 0.9494, Accuracy: 80.25%\n",
      "Epoch [161/300], Loss: 0.9371, Accuracy: 80.69%\n",
      "Epoch [162/300], Loss: 0.9218, Accuracy: 81.44%\n",
      "Epoch [163/300], Loss: 0.9202, Accuracy: 80.19%\n",
      "Epoch [164/300], Loss: 0.8973, Accuracy: 81.62%\n",
      "Epoch [165/300], Loss: 0.8987, Accuracy: 81.25%\n",
      "Epoch [166/300], Loss: 0.8919, Accuracy: 80.94%\n",
      "Epoch [167/300], Loss: 0.9014, Accuracy: 80.38%\n",
      "Epoch [168/300], Loss: 0.8787, Accuracy: 81.75%\n",
      "Epoch [169/300], Loss: 0.8589, Accuracy: 82.94%\n",
      "Epoch [170/300], Loss: 0.8550, Accuracy: 82.12%\n",
      "Epoch [171/300], Loss: 0.8421, Accuracy: 82.69%\n",
      "Epoch [172/300], Loss: 0.8348, Accuracy: 83.25%\n",
      "Epoch [173/300], Loss: 0.8345, Accuracy: 83.44%\n",
      "Epoch [174/300], Loss: 0.8172, Accuracy: 84.31%\n",
      "Epoch [175/300], Loss: 0.8062, Accuracy: 84.69%\n",
      "Epoch [176/300], Loss: 0.7875, Accuracy: 84.06%\n",
      "Epoch [177/300], Loss: 0.7876, Accuracy: 83.75%\n",
      "Epoch [178/300], Loss: 0.7982, Accuracy: 84.88%\n",
      "Epoch [179/300], Loss: 0.7650, Accuracy: 85.12%\n",
      "Epoch [180/300], Loss: 0.7792, Accuracy: 85.31%\n",
      "Epoch [181/300], Loss: 0.7523, Accuracy: 85.56%\n",
      "Epoch [182/300], Loss: 0.7555, Accuracy: 85.06%\n",
      "Epoch [183/300], Loss: 0.7573, Accuracy: 84.88%\n",
      "Epoch [184/300], Loss: 0.7417, Accuracy: 85.38%\n",
      "Epoch [185/300], Loss: 0.7276, Accuracy: 85.69%\n",
      "Epoch [186/300], Loss: 0.7129, Accuracy: 86.94%\n",
      "Epoch [187/300], Loss: 0.6985, Accuracy: 87.31%\n",
      "Epoch [188/300], Loss: 0.7001, Accuracy: 86.44%\n",
      "Epoch [189/300], Loss: 0.7018, Accuracy: 86.12%\n",
      "Epoch [190/300], Loss: 0.6831, Accuracy: 87.88%\n",
      "Epoch [191/300], Loss: 0.7025, Accuracy: 86.19%\n",
      "Epoch [192/300], Loss: 0.6755, Accuracy: 88.12%\n",
      "Epoch [193/300], Loss: 0.6556, Accuracy: 88.62%\n",
      "Epoch [194/300], Loss: 0.6660, Accuracy: 87.88%\n",
      "Epoch [195/300], Loss: 0.6324, Accuracy: 88.44%\n",
      "Epoch [196/300], Loss: 0.6485, Accuracy: 87.62%\n",
      "Epoch [197/300], Loss: 0.6157, Accuracy: 89.62%\n",
      "Epoch [198/300], Loss: 0.6178, Accuracy: 88.81%\n",
      "Epoch [199/300], Loss: 0.6242, Accuracy: 89.62%\n",
      "Epoch [200/300], Loss: 0.6226, Accuracy: 88.38%\n",
      "Epoch [201/300], Loss: 0.6187, Accuracy: 89.12%\n",
      "Epoch [202/300], Loss: 0.6067, Accuracy: 89.06%\n",
      "Epoch [203/300], Loss: 0.5931, Accuracy: 88.81%\n",
      "Epoch [204/300], Loss: 0.5918, Accuracy: 89.81%\n",
      "Epoch [205/300], Loss: 0.6000, Accuracy: 88.31%\n",
      "Epoch [206/300], Loss: 0.5757, Accuracy: 89.56%\n",
      "Epoch [207/300], Loss: 0.5900, Accuracy: 88.25%\n",
      "Epoch [208/300], Loss: 0.5772, Accuracy: 89.25%\n",
      "Epoch [209/300], Loss: 0.5522, Accuracy: 90.06%\n",
      "Epoch [210/300], Loss: 0.5586, Accuracy: 90.06%\n",
      "Epoch [211/300], Loss: 0.5429, Accuracy: 90.88%\n",
      "Epoch [212/300], Loss: 0.5374, Accuracy: 90.50%\n",
      "Epoch [213/300], Loss: 0.5085, Accuracy: 92.00%\n",
      "Epoch [214/300], Loss: 0.5170, Accuracy: 91.12%\n",
      "Epoch [215/300], Loss: 0.5460, Accuracy: 89.56%\n",
      "Epoch [216/300], Loss: 0.5151, Accuracy: 90.69%\n",
      "Epoch [217/300], Loss: 0.5133, Accuracy: 91.00%\n",
      "Epoch [218/300], Loss: 0.5150, Accuracy: 90.88%\n",
      "Epoch [219/300], Loss: 0.4925, Accuracy: 92.94%\n",
      "Epoch [220/300], Loss: 0.4912, Accuracy: 91.69%\n",
      "Epoch [221/300], Loss: 0.4768, Accuracy: 92.12%\n",
      "Epoch [222/300], Loss: 0.4714, Accuracy: 91.56%\n",
      "Epoch [223/300], Loss: 0.4652, Accuracy: 93.31%\n",
      "Epoch [224/300], Loss: 0.4884, Accuracy: 91.19%\n",
      "Epoch [225/300], Loss: 0.4616, Accuracy: 92.06%\n",
      "Epoch [226/300], Loss: 0.4576, Accuracy: 92.94%\n",
      "Epoch [227/300], Loss: 0.4564, Accuracy: 91.69%\n",
      "Epoch [228/300], Loss: 0.4641, Accuracy: 92.06%\n",
      "Epoch [229/300], Loss: 0.4315, Accuracy: 93.69%\n",
      "Epoch [230/300], Loss: 0.4343, Accuracy: 92.25%\n",
      "Epoch [231/300], Loss: 0.4211, Accuracy: 93.38%\n",
      "Epoch [232/300], Loss: 0.4306, Accuracy: 92.62%\n",
      "Epoch [233/300], Loss: 0.4275, Accuracy: 93.12%\n",
      "Epoch [234/300], Loss: 0.4481, Accuracy: 91.69%\n",
      "Epoch [235/300], Loss: 0.4074, Accuracy: 93.50%\n",
      "Epoch [236/300], Loss: 0.4131, Accuracy: 93.06%\n",
      "Epoch [237/300], Loss: 0.3969, Accuracy: 93.19%\n",
      "Epoch [238/300], Loss: 0.3730, Accuracy: 94.25%\n",
      "Epoch [239/300], Loss: 0.4109, Accuracy: 92.94%\n",
      "Epoch [240/300], Loss: 0.3936, Accuracy: 94.06%\n",
      "Epoch [241/300], Loss: 0.3733, Accuracy: 94.31%\n",
      "Epoch [242/300], Loss: 0.3818, Accuracy: 94.81%\n",
      "Epoch [243/300], Loss: 0.3777, Accuracy: 94.00%\n",
      "Epoch [244/300], Loss: 0.3718, Accuracy: 94.25%\n",
      "Epoch [245/300], Loss: 0.3753, Accuracy: 92.94%\n",
      "Epoch [246/300], Loss: 0.3656, Accuracy: 94.12%\n",
      "Epoch [247/300], Loss: 0.3638, Accuracy: 94.06%\n",
      "Epoch [248/300], Loss: 0.3538, Accuracy: 94.62%\n",
      "Epoch [249/300], Loss: 0.3768, Accuracy: 93.62%\n",
      "Epoch [250/300], Loss: 0.3540, Accuracy: 94.31%\n",
      "Epoch [251/300], Loss: 0.3458, Accuracy: 94.06%\n",
      "Epoch [252/300], Loss: 0.3398, Accuracy: 94.69%\n",
      "Epoch [253/300], Loss: 0.3292, Accuracy: 95.44%\n",
      "Epoch [254/300], Loss: 0.3432, Accuracy: 95.12%\n",
      "Epoch [255/300], Loss: 0.3256, Accuracy: 94.94%\n",
      "Epoch [256/300], Loss: 0.3245, Accuracy: 95.00%\n",
      "Epoch [257/300], Loss: 0.3190, Accuracy: 95.44%\n",
      "Epoch [258/300], Loss: 0.3121, Accuracy: 96.12%\n",
      "Epoch [259/300], Loss: 0.3173, Accuracy: 95.12%\n",
      "Epoch [260/300], Loss: 0.3117, Accuracy: 95.44%\n",
      "Epoch [261/300], Loss: 0.3067, Accuracy: 94.88%\n",
      "Epoch [262/300], Loss: 0.3066, Accuracy: 95.44%\n",
      "Epoch [263/300], Loss: 0.2968, Accuracy: 95.25%\n",
      "Epoch [264/300], Loss: 0.3006, Accuracy: 95.06%\n",
      "Epoch [265/300], Loss: 0.2831, Accuracy: 96.06%\n",
      "Epoch [266/300], Loss: 0.2825, Accuracy: 95.62%\n",
      "Epoch [267/300], Loss: 0.2834, Accuracy: 95.62%\n",
      "Epoch [268/300], Loss: 0.2895, Accuracy: 95.88%\n",
      "Epoch [269/300], Loss: 0.2835, Accuracy: 95.56%\n",
      "Epoch [270/300], Loss: 0.2820, Accuracy: 95.81%\n",
      "Epoch [271/300], Loss: 0.2733, Accuracy: 96.25%\n",
      "Epoch [272/300], Loss: 0.2814, Accuracy: 95.75%\n",
      "Epoch [273/300], Loss: 0.2751, Accuracy: 95.56%\n",
      "Epoch [274/300], Loss: 0.2487, Accuracy: 97.06%\n",
      "Epoch [275/300], Loss: 0.2559, Accuracy: 96.38%\n",
      "Epoch [276/300], Loss: 0.2579, Accuracy: 96.44%\n",
      "Epoch [277/300], Loss: 0.2458, Accuracy: 97.12%\n",
      "Epoch [278/300], Loss: 0.2439, Accuracy: 96.75%\n",
      "Epoch [279/300], Loss: 0.2462, Accuracy: 97.44%\n",
      "Epoch [280/300], Loss: 0.2624, Accuracy: 96.25%\n",
      "Epoch [281/300], Loss: 0.2444, Accuracy: 96.88%\n",
      "Epoch [282/300], Loss: 0.2282, Accuracy: 97.31%\n",
      "Epoch [283/300], Loss: 0.2414, Accuracy: 96.62%\n",
      "Epoch [284/300], Loss: 0.2416, Accuracy: 96.38%\n",
      "Epoch [285/300], Loss: 0.2262, Accuracy: 97.00%\n",
      "Epoch [286/300], Loss: 0.2262, Accuracy: 96.75%\n",
      "Epoch [287/300], Loss: 0.2198, Accuracy: 96.69%\n",
      "Epoch [288/300], Loss: 0.2202, Accuracy: 97.44%\n",
      "Epoch [289/300], Loss: 0.2226, Accuracy: 97.06%\n",
      "Epoch [290/300], Loss: 0.2237, Accuracy: 96.31%\n",
      "Epoch [291/300], Loss: 0.2061, Accuracy: 97.62%\n",
      "Epoch [292/300], Loss: 0.2081, Accuracy: 97.38%\n",
      "Epoch [293/300], Loss: 0.1994, Accuracy: 97.44%\n",
      "Epoch [294/300], Loss: 0.2182, Accuracy: 96.62%\n",
      "Epoch [295/300], Loss: 0.2087, Accuracy: 97.00%\n",
      "Epoch [296/300], Loss: 0.2033, Accuracy: 96.94%\n",
      "Epoch [297/300], Loss: 0.1965, Accuracy: 97.19%\n",
      "Epoch [298/300], Loss: 0.1934, Accuracy: 97.81%\n",
      "Epoch [299/300], Loss: 0.1884, Accuracy: 97.81%\n",
      "Epoch [300/300], Loss: 0.1993, Accuracy: 97.38%\n",
      "\n",
      "Evaluating CRNN...\n",
      "CRNN Test Accuracy: 33.75%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_type = \"mel_spectrogram\"\n",
    "feature_dir = f\"processed_data/{feature_type}\"\n",
    "metadata = pd.read_csv(\"dataset/esc50.csv\")\n",
    "num_classes = len(metadata[\"category\"].unique())\n",
    "num_epochs = 300\n",
    "# Create dataset and dataloaders\n",
    "dataset = AudioDataset(metadata, feature_type, feature_dir, num_classes)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CRNN(input_channels=1, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)  # Increase weight decay\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=1e-3, steps_per_epoch=len(train_loader), epochs=num_epochs\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, 1)  # Get class predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_accuracy = (correct / total) * 100\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate CRNN\n",
    "print(\"\\nEvaluating CRNN...\")\n",
    "crnn_accuracy = evaluate_model(model, test_loader, device)\n",
    "print(f\"CRNN Test Accuracy: {crnn_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jimmy_coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
